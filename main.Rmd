---
title: "Main"
author: "Aniol, Omar, Marcel Aranich"
date: "2026-01-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("tidyr")
# install.packages("mltools")
# install.packages("data.table")
# install.packages("lubridate")
# install.packages("ggplot2")
# install.packages("gridExtra")

library(dplyr)
library(tidyr)
library(stringr)
library(mltools)
library(data.table)
library(lubridate)
library(ggplot2)
library(gridExtra)
```

## 2. Análisis de logs de servidor usando R (parte II)

### Obtención y carga de los Datos:

> Queremos programar un script con el que podamos hacer una investigación forense sobre un fichero de logs de un servidor de tipo Apache. Los datos del registro del servidor están en el formato estándar e incluyen miles de registros sobre las distintas peticiones gestionadas por el servidor web.
>
> Nuestro programa ha de ser capaz de obtener las respuestas de forma dinámica a las siguientes preguntas utilizando instrucciones de código en R:

#### 1.

> Descomprimir el fichero comprimido que contiene los registros del servidor, y a partir de los datos extraídos, cargar en data frame los registros con las peticiones servidas.

```{r, cache=TRUE}
logs <- read.table("epa-http.csv",
                   header = FALSE,
                   sep = " ",
                   quote = "\"",
                   stringsAsFactors = FALSE,
                   fill = TRUE)


```

#### 2.

> Incluid en el documento un apartado con la descripción de los datos analizados: fuente, tipología, descripción de la información contenida (los diferentes campos) y sus valores.

Los datos analizados son entradas de logs que muestra la información de peticiones hechas en lo que puede ser una página o aplicación web.

Hay 7 campos claramente diferenciados:

-   **Host:** Muestra quién ha hecho la petición. Es una variable cualitativa y sus valores van desde direcciones IP hasta direcciones web.

-   **Timestamp:** Muestra cuándo se hizo la petición; específicamente indica día, hora, minutos y segundos.

-   **Method:** Muestra cuál ha sido el método de la petición. Es una variable cualitativa y puede tomar los valores GET, POST y HEAD.

-   **Resource:** Muestra cuál es el recurso usado en la petición. Es una variable cualitativa y sus valores corresponden a distintos tipos de ficheros.

-   **Protocol:** Muestra el protocolo de la petición. Es una variable cualitativa y puede tomar los valores HTTP/1.0 y HTTP/0.2.

-   **Status:** Muestra el código del resultado de la petición. Es una variable cualitativa y puede tomar valores como 200, 404, etc.

-   **Size:** Muestra el tamaño de la petición. Es una variable cuantitativa y sus valores van desde 0 hasta números bastante grandes.

### Limpieza de los Datos

#### 3.

> Aprovechando que los datos a analizar son los mismos de la primera práctica, para esta entrega es imprescindible que los datos estén en formato de “datos elegantes”.
>
> Esto incluye la correcta codificación de cada columna de los datos en el tipo adecuado según su naturaleza (numérico, cadena de caracteres, lógico, timestamp, factor, etc.). Además, para esta entrega se valorará también que los datos estén limpios y sin elementos extraños como por ejemplo espacios o signos de puntuación no necesarios.

Para limpiar los datos primero hemos nombrado las 5 columnas que se han generado al extraer los datos del documento.

Usando la función "separate" hemos separado el método, el recurso y el protocolo que se habían extraído juntos en una sola variable.

Para limpiar el "timestamp" primero ha sido necesario eliminar los "[]" con los cuáles venían los datos. Una vez eliminado los corchetes hemos transformado la variable al tipo POSIXct para poder guardarla de la forma más eficiente y manejable posible.

Finalmente, los últimos cambios que hemos hecho para limpiar los datos han sido transformar la variable status a un integer para despues transformarla a un factor. Este último paso también lo hemos hecho con las variables de método y protocolo.

```{r, cache=TRUE}

colnames(logs) <- c("host", "timestamp", "resource", "status", "size")

logs <- tidyr::separate(logs, resource, c("method", "resource", "protocol"), sep = " ")

logs$method <- as.factor(logs$method)

logs$protocol <- as.factor(logs$protocol)

logs$timestamp <- gsub("\\[", "", logs$timestamp)
logs$timestamp <- gsub("]$", "", logs$timestamp)
logs$timestamp <- as.POSIXct(logs$timestamp, format = "%d:%H:%M:%OS")


logs$status <- as.integer(logs$status)
logs$status <- as.factor(logs$status)

suppressWarnings(
logs$size <- as.integer(logs$size)
)


head(logs)


```

### Exploración de Datos

#### 4.

> Identificar el *número único de usuarios* que han interactuado directamente con el servidor de forma segregada según si los usuarios han tenido algún tipo de error en las distintas peticiones ofrecidas por el servidor.
>
> Hacer la distinción (break down) del número de usuarios en función de si estos han tenido algún tipo de error durante las interacciones con el servidor y el tipo de error, es decir, ofrecer el número de usuarios que no han tenido ningún error en una de las peticiones gestionadas por el servidor y el caso contrario, el número de usuarios que sí han experimentado algún error para una petición servida según la tipología del error.
>
> Describir en el documento los distintos tipos de errores existentes en la muestra de datos y el número único de usuarios.

Para hacer el ejercicio hemos combinado las funciones `group_by` y `summarise` para obtener un dataframe que muestra los distintos status y el número de peticiones con cada status.

```{r}

df_4 <- logs %>% 
  group_by(status) %>% 
  summarise(comptador = n()) %>% 
  arrange(desc(comptador))

head(df_4)

```

El significado de los status son los siguientes:

-   200: Petición correcta.
-   304: No modificado.
-   302: Redirección temporal.
-   404: No encontrado.
-   403: Prohibido.
-   501: No implementado

### Análisis de Datos

#### 5.

> Analizar los distintos tipos de peticiones HTTP (GET, POST, PUT, DELETE) gestionadas por el servidor, identificando la frecuencia de cada una de estas. Repetir el análisis, esta vez filtrando previamente aquellas peticiones correspondientes a recursos ofrecidos de tipo imagen.

Para hacer este ejercicio hemos usado la combinación de group_by por metodo y de summarise para poder obtener los distintos métodos y cuantas peticiones han usado cada método.

```{r}
df_5_1 <- logs %>% 
  group_by(method) %>% 
  summarise(comptador = n()) %>% 
  arrange(desc(comptador))

head(df_5_1)
```

Para esta versión hemos creado un data frame que tiene como variable el tipo de fichero del recurso. A este data frame hemos dejado solo las entradas las cuáles el recurso era una imagen y hemos repetido el mismo proceso del apartado anterior.

```{r}
imatges <- logs %>% mutate(extensio = str_extract(resource, "(?<=\\.)[^\\.]+$"))

extensions_imatge <- c("jpg", "jpeg", "png", "gif", "bmp", "svg", "ico", "GIF", "JPG")

imatges <- imatges %>%
  filter(extensio %in% extensions_imatge)

df_5_2 <- imatges %>%
  group_by(method) %>%
  summarise(comptador = n()) %>%
  arrange(desc(comptador))

head(df_5_2)

```

### Visualización de Resultados

#### 6.

> Generar al menos 2 gráficos distintos que permitan visualizar alguna característica relevante de los datos analizados.
>
> Estos deberán representar por lo menos 1 o 2 variables diferentes del data frame. Describid el gráfico e indicad cualquier observación destacable que se pueda apreciar gracias a la representación gráfica.

```{r}
df_summary <- logs %>% count(method, status)
ggplot(df_summary, aes(x = method, y = n, fill = status)) +
  geom_col(position = "dodge") +
  scale_y_log10() +
  labs(
    title = "Número de status por método",
    x = "Método",
    y = "Cantidad",
    fill = "Status"
  ) +
  theme_minimal()

```

Este gráfico muestra el status de las peticiones dependiendo del método. Cada status está representado por un color distinto y la altura de la barra especifica el número de peticiones. En este gráfico en específico resulta interesante el método HEAD, ya que todas las peticiones con este método han resultado siempre con un valor 200. También podemos destacar como GET al ser el método más usado es el que tiene más valores de status distintos.

```{r}
df_t <- logs
df_t$size <- df_t$size + 1
df_t <- df_t %>% drop_na()
ggplot(df_t, aes(x = size)) +
  geom_histogram(aes(y = after_stat(count + 1)), bins = 30) +
  scale_y_log10() +
  theme_minimal()


```

Este histograma muestra la frecuencia del tamaño de las peticiones. Como había algunas peticiones con size "indeterminado" estas no han sido consideradas para la creación del histograma. De aquí se pueden ver dos cosas interesantes, la primera es la alta frecuencia de metodos con tamaño "pequeño" en comparación con todo el resto. La segunda es como hay pocas peticiones que tienen un tamaño exageradamente grande en comparación con el resto de peticiones, miden casi el doble.

#### 7.

> Generar un gráfico que permita visualizar el número de peticiones servidas a lo largo del tiempo.
>
> Pista: Es imprescindible haber codificado correctamente el tipo de cada columna, y en particular, el de la columna usada para representar el momento en que se sirve la respuesta. Con un formato adecuado, será más fácil potencialmente extraer nuevas columnas derivadas de la original que puedan ayudar en la representación de la información (día, hora, etc.)

Para generar el gráfico hemos usado la variable timestamp de nuestro dataset. Hemos creado un dataframe que guarda por hora el número de peticiones recibidas. Una vez creado, hemos usado ggplot para generar un gráfico que muestra de forma visual la cantidad de peticiones por cada hora.

```{r}
logs_time <- logs %>%
  mutate(hora = floor_date(timestamp, unit = "hour")) %>%
  group_by(hora) %>%
  summarise(peticiones = n())

ggplot(logs_time, aes(x = hora, y = peticiones)) +
  geom_line(color = "steelblue", linewidth = 1) +
  labs(
    title = "Número de peticiones a lo largo del tiempo",
    x = "Tiempo",
    y = "Número de peticiones"
  ) +
  theme_minimal()


```

## Clústering de datos

#### 8.

> Utilizando un algoritmo de aprendizaje no supervisado, realizad un análisis de clústering con k-means para los datos del servidor.
>
> -   Para este análisis debéis repetir la ejecución del modelado con distintos valores de k (número de clústeres) con al menos 2 valores diferentes de k.
> -   A fin de retener algo de información sobre el recurso servido, generad una columna numérica derivada de esta con el número de caracteres de la URL servida.

El algoritmo de K-menas necessita usar datos numéricos, así que tenemos que transdormar las columnas **resources**, **size**, **method** y **status** a datos numéricos.

Para processar la columna de **resources**, usaremos el tipo de recurso que se está intentando obtener como un factor. Por ejemplo para `/icons/circle_logo_small.gif` -\> `gif`. Si no hay ningún tipo, entonces usaremos `None` como un factor para indicar eso. Existen casos dónde el tipo que obtenemos no tiene sentido, pero és un problema que hemos decidido aceptar.

```{r warning=FALSE}

# -   host: discard - no useful data
# -   timestamp: normalización 0-1 (t - min / (max - min))
# -   method: one hot encoding
# -   resource: discard
# -   protocol: discard - all are equal but 1 outlier
# -   status: one hot encoding
# -   size: use (log scale ? + normalize 0 1) + discard NA

max_length_type <- 10

df <- tidyr::separate(
  logs, 
  resource, 
  c("resource_1", "type"), 
  sep="(?=\\.[^.]+$)", 
  remove = FALSE, 
  extra = "merge", 
  fill = "right")

df$type[is.na(df$type)] <- "None"
df <- df %>% mutate(type = if_else(nchar(type) > max_length_type, "None", type))
df$type <- as.factor(df$type)

```

Para el siguiente paso, procesaremos **size**. Para empezar, existen algunos valores que són `NA`, por lo que tenemos que gestionarlos. Para eso hay 2 opciones:

-   Descartar las entradas que contengan `NA`.
-   Dar un valor por defecto a los `NA`, cómo 0 o el valor medio del dataset.

Hemos decidido descartar las filas con `NA`.

Ahora que tenemos un tipo púramente numérico, podemos trabajar con él. Si miramos algunos de los datos podemos ver que los datos abarcan múltiples órdenes de magnitud, por lo que tiene sentido processsarlos en una escala logarítmica, particularmente sabiendo que solo hay datos no-negativos.

Optional: Además, aplicaremos la normalización min-max.

```{r}

df_km <- df %>% drop_na()

df_km$size <- log1p(df_km$size)

if (FALSE) {
  
  size_min <- min(df_km$size) # 0.0
  size_max <- max(df_km$size)
  
  df_km$size <- (df_km$size - size_min) / (size_max - size_min)
}

# Opcional: multiplicar por constatne para ajustar tamaños
# df_km$size <- df_km$size * 2.5


```

Ahora procesamos **method** y **status**. Para transformarlos a datos numéricos usaremos one hot encoding. Hacemos lo mismo para la columna **type** y descartamos as columnas que no necesitamos.

```{r}



df_km <- data.table(df_km)

df_km <- one_hot(df_km, cols = "method")

df_km <- one_hot(df_km, cols = "status")

df_km <- one_hot(df_km, cols = "type")


df_km <- df_km %>% select(-one_of('host', 'timestamp', 'resource', 'resource_1', 'protocol'))

# Por si acaso
df_km <- df_km %>% drop_na()
df <- df %>% drop_na()

```

Preparamos algunas variables para la ejecución del K-means.

```{r, cache=TRUE}
SEED <- 3

set.seed(SEED)


clusters <- c(2, 3, 5)
len_clusters = length(clusters)
max_iter = 250
```

Ejecutamos el algoritmo K-means.

```{r, cache=TRUE}

results <- lapply(clusters, function(k) kmeans(df_km, centers = k, nstart = 25))

```

#### 9.

> Representad visualmente en gráficos de tipo scatter plot el resultado de vuestros clústering y interpretad el resultado obtenido.
>
> (describid las características de los distintos grupos) con los 2 valores distintos de k probados en el apartado anterior en función de los valores de las variables y el número de clúster asignado.

Queremos visualizar:

-   Type (images) vs Size
-   Size vs status code
-   Method vs size

Primeramente creamos una variable para diferenciar entre las imagenes y el resto del contenido.

```{r}


# Los formatos no usados no estan porque aparecen 0 veces. 
# extensions_imatge <- c("jpg", "jpeg", "png", "gif", "bmp", "svg", "ico", "GIF")
is_image_idx <- df_km$type_.jpg == 1 |
  df_km$type_.JPG == 1 |
  df_km$type_.gif == 1 |
  df_km$type_.GIF == 1 |
  df_km$type_.ico == 1

df_km$is_image <- is_image_idx
df$is_image <- is_image_idx

palette = c("#E0614A", "#5D3CE0", "#E0D04A", "#4AE0A2", "#8B865C", "#615350")

```

```{r}
df_current <- df_km %>% select(one_of("size", "is_image"))

total_graphs <- list()

for (j in 1:len_clusters) {
  current_result <- results[[j]]
  group_labels <- current_result$cluster
  df_current$group <- group_labels

  k <- clusters[j]  
  par(mfrow = c(k, 1))
  graphs <- list()
  
  for (i in 1:k) {
    current = df_current[df_current$group == i]
    label = sprintf("From group %d, is an image?\n%d clusters", i, k)
    # boxplot(size ~ is_image, data = df_current, xlab = label, col = palette[i])
    
    graph <- ggplot(df_current, aes(x = is_image, y = size, fill = is_image)) + 
      geom_boxplot() + 
      labs(x = label, y = "size") + 
      scale_fill_manual(values = palette)
    graphs[[i]] = graph
  }
  total_graphs[[j]] = graphs
}

```

Ahora mostramos los gráficos.

```{r}
# k = 2
joined_2 <- grid.arrange(grobs = total_graphs[[1]], ncol=2) # error here
print(joined_2)

# k = 3
joined_3 <- grid.arrange(grobs = total_graphs[[2]], ncol=3) # error here
print(joined_3)

# k = 5
joined_5 <- grid.arrange(grobs = total_graphs[[3]], ncol=3, nrow = 2) # error here
print(joined_5)

```

*(Nota: no sabemos porqué se imprimen también algunos dataframes)*

Desafortunadamente, no podemos ver mucha diferéncia entre los grupos. Dónde podemos apreciar una diferencia és en el hecho de que cuándo se envia una imagen, el `size` és menor.

Podemos repetir un analisis similar, pero analizar `size` contra `status_code`.

```{r}

palette_2 <- c("#F09224", "#7329F0", "#17F042", "#F0362B", "#2991F0", "#E0F018", "#704643", "#435B70")

status_codes = c("status_200", "status_302", "status_304", "status_400", "status_403", "status_404", "status_500", "status_501")

df_current <- df %>% select(one_of("status", "size"))


total_graphs <- list()

for (j in 1:len_clusters) {
  k <- clusters[j]  
  current_result <- results[[j]]
  df_current$group <- current_result$cluster
  
  graphs <- list()
  
  
  for (i in 1:k) {
    label <- sprintf("Status code in group %d\nFor %d clusters", i, k)
    
    current = df_current[df_current$group == i, ]
    
    summary_df <- current %>% 
      group_by(status) %>% 
      summarise(total_value = sum(size), .groups = "drop") %>% 
      complete(status, fill = list(total_value = 0))
    
    graph <- ggplot(summary_df, aes(x = status, y = total_value + 2, fill = palette_2)) + 
      scale_y_log10() + 
      stat_summary(fun = "sum", geom = "bar") + 
      labs(x = label, y = "size") + 
      scale_fill_manual(values = palette_2) + 
      theme(legend.position = "top")
      # theme_minimal() 
    
    # graph <- ggplot(df_current[df_current[group == i]], aes(y = size)) + 
    #   geom_bar() + 
    graphs[[i]] = graph
  }
  total_graphs[[j]] = graphs
}


```

```{r}
# k = 2
joined_2 <- grid.arrange(grobs = total_graphs[[1]], ncol=2)
print(joined_2)

# k = 3
joined_3 <- grid.arrange(grobs = total_graphs[[2]], ncol=3)
print(joined_3)

# k = 5
joined_5 <- grid.arrange(grobs = total_graphs[[3]], ncol=3, nrow = 2)
print(joined_5)

```

TODO: conclusiones

Replicamos el proceso nuevamente para **method**.

```{r}

df_current <- df %>% select(one_of("method", "size"))

palette_3 <- c("#F09224", "#7329F0", "#17F042")
total_graphs <- list()

for (j in 1:len_clusters) {
  k <- clusters[j]  
  current_result <- results[[j]]
  df_current$group <- current_result$cluster
  
  graphs <- list()
  
  
  for (i in 1:k) {
    label <- sprintf("Method in group %d\nFor %d clusters", i, k)
    
    current = df_current[df_current$group == i, ]
    
    summary_df <- current %>% 
      group_by(method) %>% 
      summarise(total_value = sum(size), .groups = "drop") %>% 
      complete(method, fill = list(total_value = 0))
    
    graph <- ggplot(summary_df, aes(x = method, y = total_value + 2, fill = palette_3)) + 
      stat_summary(fun = "sum", geom = "bar") + 
      scale_y_log10() + 
      labs(x = label, y = "size") + 
      # theme_minimal() + 
      theme(legend.position = "top")
    
    # graph <- ggplot(df_current[df_current[group == i]], aes(y = size)) + 
    #   geom_bar() + 
    graphs[[i]] = graph
  }
  total_graphs[[j]] = graphs
}

```

```{r}
# k = 2
joined_2 <- grid.arrange(grobs = total_graphs[[1]], ncol=2)
print(joined_2)

# k = 3
joined_3 <- grid.arrange(grobs = total_graphs[[2]], ncol=3)
print(joined_3)

# k = 5
joined_5 <- grid.arrange(grobs = total_graphs[[3]], ncol=3, nrow = 2)
print(joined_5)

```

TODO: conclusiones

```{r}
```
